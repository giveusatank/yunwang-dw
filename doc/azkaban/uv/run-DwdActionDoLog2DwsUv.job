#run-DwdActionDoLog2DwsUv.job
type=command
command=sh /usr/local/spark-2.2.3-bin-hadoop2.6/bin/spark-submit \
 --master yarn \
 --deploy-mode cluster \
 --num-executors 10 \
 --executor-memory 3G \
 --driver-memory 1G \
 --executor-cores 2 \
 --queue dwq \
 --conf spark.yarn.maxAppAttempts=0 \
 --files /usr/local/spark-2.2.3-bin-hadoop2.6/conf/hive-site.xml \
 --class com.pep.dws.uv.DwdActionDoLog2DwsUv \
 /usr/local/spark-2.2.3-bin-hadoop2.6/pep-job/yunwang-dw-1.0-SNAPSHOT.jar
working.dir=/usr/local/azkaban/azkaban_work_dir/working_dir




spark:SparkSession

spark.sql() -> Dataset[Row]
spark.read.jdbc

spark.read.format().load()
df.write.format().save()

df.write.SaveMode().jdbc()



insert into ads.ads_textbook_used_total partition (count_date)
select product_id,
       company,
       sum(sum_time_consume),
       sum(sum_time_consume) / sum(start_action_count),
       avg(avg_time_consume) * sum(action_count),
       sum(start_action_count),
       sum(action_count)         as pv,
       count(distinct (user_id)) as uv,
       count_date,
       '20191017'
from dws.dws_textbook_used_total where count_date='20191017'
group by count_date, product_id, company



byte short char int long float double boolean

boolean - 1
byte - 1 
char - 2
short - 2
int - 4
long - 8
float - 4
double - 8

以上八种基本数据类型以及引用类型是程序比如Java中识别的，
而计算机存储不会识别这些类型，只会存储字符对应的码值所对应的的
补码。
而程序读取存储的数据的时候只是将字节流读出，以何种解码方式解码
可以通过程序中设置。解码之后是字符串，然后也可以将该字符串转为基本类型。

